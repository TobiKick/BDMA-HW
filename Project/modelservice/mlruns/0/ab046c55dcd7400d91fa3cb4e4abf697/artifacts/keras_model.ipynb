{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Model, Sequential\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "# from keras.layers import Dense, BatchNormalization, Dropout, Flatten, Input, GaussianNoise\n",
    "import tensorflow as tf\n",
    "# from keras import regularizers, backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers, backend as K\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import mlflow.keras\n",
    "import mlflow\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED) # NumPy\n",
    "random.seed(RANDOM_SEED) # Python\n",
    "tf.set_random_seed(RANDOM_SEED) # Tensorflow\n",
    "\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/pawelurbanowicz/master-thesis/data/'\n",
    "train = pd.read_csv(f'{folder_path}train_transaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration \n",
    "\n",
    "### Transaction\n",
    "\n",
    "features:\n",
    "* TransactionID - unique id \n",
    "* isFraud - value to predict\n",
    "* TransactionDT - timedelta from a given reference datetime (not an actual timestamp)\n",
    "* TransactionAmt - amount in USD\n",
    "* dist1,dist2 - distance\n",
    "* C1 ... C14 - counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked\n",
    "* D1 ... D15 - timedelta, such as days between previous transaction\n",
    "* V1 ... V339 - vesta engineered rich features\n",
    "\n",
    "categorical_features: \n",
    "* ProductCD - product code \n",
    "* P_emaildomain,R_emaildomain - purchaser and recipient email domain\n",
    "* card1 ... card6 - payment card information\n",
    "* addr1,addr2 - address(region, country)\n",
    "* M1 ... M9 - match, such as names on card and address\n",
    "\n",
    "https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-671062\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "base_features = ['TransactionID', 'isFraud', 'TransactionDT']\n",
    "\n",
    "c_counting_features = ['C' + str(n) for n in range(1,15)]\n",
    "d_timedelta_features = ['D' + str(n) for n in range(1,16)]\n",
    "numerical_features = ['TransactionAmt', 'dist1', 'dist2'] + c_counting_features + d_timedelta_features \n",
    "\n",
    "vesta_features = ['V' + str(n) for n in range(1,340)]\n",
    "\n",
    "# categorical_features\n",
    "m_features = ['M' + str(n) for n in range(1,10)]\n",
    "card_features = ['card' + str(n) for n in range(1,7)]\n",
    "categorical_features = ['ProductCD','P_emaildomain','R_emaildomain', 'addr1','addr2'] + m_features + card_features\n",
    "\n",
    "# all features\n",
    "all_features = base_features + numerical_features + vesta_features + categorical_features\n",
    "\n",
    "# train = train[base_features + categorical_features + numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (590540, 394)\n",
      "Number of fraud txs:  20663\n",
      "Number of normal txs:  569877\n",
      "Procentage of fraud txs:  3.4990009144173126\n"
     ]
    }
   ],
   "source": [
    "frauds = train[train.isFraud == 1]\n",
    "normal = train[train.isFraud == 0]\n",
    "print('Shape: ', train.shape)\n",
    "print(\"Number of fraud txs: \", len(frauds))\n",
    "print(\"Number of normal txs: \", len(normal))\n",
    "print(\"Procentage of fraud txs: \", len(frauds)/len(train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "\n",
       "   card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
       "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "\n",
       "  V336  V337  V338  V339  \n",
       "0  NaN   NaN   NaN   NaN  \n",
       "1  NaN   NaN   NaN   NaN  \n",
       "2  NaN   NaN   NaN   NaN  \n",
       "\n",
       "[3 rows x 394 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionAmt         0\n",
       "dist1             352271\n",
       "dist2             552913\n",
       "C1                     0\n",
       "C2                     0\n",
       "C3                     0\n",
       "C4                     0\n",
       "C5                     0\n",
       "C6                     0\n",
       "C7                     0\n",
       "C8                     0\n",
       "C9                     0\n",
       "C10                    0\n",
       "C11                    0\n",
       "C12                    0\n",
       "C13                    0\n",
       "C14                    0\n",
       "D1                  1269\n",
       "D2                280797\n",
       "D3                262878\n",
       "D4                168922\n",
       "D5                309841\n",
       "D6                517353\n",
       "D7                551623\n",
       "D8                515614\n",
       "D9                515614\n",
       "D10                76022\n",
       "D11               279287\n",
       "D12               525823\n",
       "D13               528588\n",
       "D14               528353\n",
       "D15                89113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[numerical_features].isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for  ProductCD 5\n",
      "Unique values for  P_emaildomain 60\n",
      "Unique values for  R_emaildomain 61\n",
      "Unique values for  addr1 333\n",
      "Unique values for  addr2 75\n",
      "Unique values for  M1 3\n",
      "Unique values for  M2 3\n",
      "Unique values for  M3 3\n",
      "Unique values for  M4 4\n",
      "Unique values for  M5 3\n",
      "Unique values for  M6 3\n",
      "Unique values for  M7 3\n",
      "Unique values for  M8 3\n",
      "Unique values for  M9 3\n",
      "Unique values for  card1 13553\n",
      "Unique values for  card2 501\n",
      "Unique values for  card3 115\n",
      "Unique values for  card4 5\n",
      "Unique values for  card5 120\n",
      "Unique values for  card6 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProductCD             0\n",
       "P_emaildomain     94456\n",
       "R_emaildomain    453249\n",
       "addr1             65706\n",
       "addr2             65706\n",
       "M1               271100\n",
       "M2               271100\n",
       "M3               271100\n",
       "M4               281444\n",
       "M5               350482\n",
       "M6               169360\n",
       "M7               346265\n",
       "M8               346252\n",
       "M9               346252\n",
       "card1                 0\n",
       "card2              8933\n",
       "card3              1565\n",
       "card4              1577\n",
       "card5              4259\n",
       "card6              1571\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in categorical_features:\n",
    "    print(\"Unique values for \", col, len(train[col].unique()))\n",
    "    \n",
    "train[categorical_features].isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation\n",
    "\n",
    "https://www.kaggle.com/abazdyrev/keras-nn-focal-loss-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.sort_values('TransactionDT').drop(['isFraud'], axis=1)\n",
    "y = train.sort_values('TransactionDT')['isFraud'].to_numpy()\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tranformers import drop_columns, CategoricalTransformer, NumericalTransformer\n",
    "\n",
    "\n",
    "def get_categories(df, n_values = 50):\n",
    "    categories = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        categories.append(list(df[column].value_counts().iloc[: n_values - 1].index) + ['other'])\n",
    "        \n",
    "    return categories\n",
    "\n",
    "categories = get_categories(X[categorical_features])\n",
    "\n",
    "categorical_pipe = make_pipeline(\n",
    "    CategoricalTransformer(categories), \n",
    "    OneHotEncoder(categories = categories, sparse = False )\n",
    ")\n",
    "\n",
    "numerical_pipe = make_pipeline(\n",
    "    NumericalTransformer(), \n",
    "    StandardScaler(), \n",
    "    SimpleImputer(strategy='constant', fill_value=0, missing_values=np.nan)\n",
    ")\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (FunctionTransformer(drop_columns), ['TransactionDT', 'TransactionID'] + vesta_features),\n",
    "    (categorical_pipe, categorical_features),\n",
    "    (numerical_pipe, numerical_features),\n",
    ")\n",
    "\n",
    "X = transformer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model building\n",
    "\n",
    "Based on https://arxiv.org/pdf/1908.11553.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/10\n",
      "472432/472432 [==============================] - 28s 60us/sample - loss: 0.0990 - acc: 0.9726 - val_loss: 0.0928 - val_acc: 0.9736\n",
      "Epoch 2/10\n",
      "472432/472432 [==============================] - 33s 69us/sample - loss: 0.0896 - acc: 0.9747 - val_loss: 0.0880 - val_acc: 0.9747\n",
      "Epoch 3/10\n",
      "472432/472432 [==============================] - 35s 73us/sample - loss: 0.0840 - acc: 0.9761 - val_loss: 0.0849 - val_acc: 0.9764\n",
      "Epoch 4/10\n",
      "472432/472432 [==============================] - 33s 71us/sample - loss: 0.0802 - acc: 0.9772 - val_loss: 0.0807 - val_acc: 0.9770\n",
      "Epoch 5/10\n",
      "472432/472432 [==============================] - 35s 75us/sample - loss: 0.0766 - acc: 0.9781 - val_loss: 0.0785 - val_acc: 0.9779\n",
      "Epoch 6/10\n",
      "472432/472432 [==============================] - 36s 76us/sample - loss: 0.0738 - acc: 0.9787 - val_loss: 0.0781 - val_acc: 0.9776\n",
      "Epoch 7/10\n",
      "472432/472432 [==============================] - 51s 109us/sample - loss: 0.0711 - acc: 0.9793 - val_loss: 0.0740 - val_acc: 0.9789\n",
      "Epoch 8/10\n",
      "472432/472432 [==============================] - 37s 79us/sample - loss: 0.0690 - acc: 0.9799 - val_loss: 0.0719 - val_acc: 0.9793\n",
      "Epoch 9/10\n",
      "472432/472432 [==============================] - 36s 76us/sample - loss: 0.0669 - acc: 0.9804 - val_loss: 0.0712 - val_acc: 0.9799\n",
      "Epoch 10/10\n",
      "440320/472432 [==========================>...] - ETA: 2s - loss: 0.0651 - acc: 0.9809"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "def make_model():\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(500, activation='tanh'))\n",
    "    model.add(Dropout(.1))\n",
    "    model.add(Dense(250, activation='tanh'))\n",
    "    model.add(Dropout(.1))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(.1))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    validation_data=(X_test, y_test), \n",
    "    epochs=EPOCHS,\n",
    "    shuffle=True,\n",
    "    batch_size=128).history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "* Confusion matrix, precision, recall and F1\n",
    "* ROC and AUROC\n",
    "\n",
    "Accuracy is misleading for this dateset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.3)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"Test Data Accuracy:\", score)\n",
    "\n",
    "stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "mlflow.log_metric(\"precision 1\", stats[0][1])\n",
    "mlflow.log_metric(\"recall 1\", stats[1][1])\n",
    "mlflow.log_metric(\"f1-score 1\", stats[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (4,4))\n",
    "\n",
    "sns.heatmap(df_cm, annot=True, fmt='g', xticklabels=LABELS, yticklabels=LABELS)\n",
    "\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()\n",
    "\n",
    "mlflow.log_metric(\"Tnormal-Pnormal\", cm[0][0])\n",
    "mlflow.log_metric(\"Tnormal-Pfraud\", cm[0][1])\n",
    "mlflow.log_metric(\"Tfraud-Pnormal\", cm[1][0])\n",
    "mlflow.log_metric(\"Tfraud-Pfraud\", cm[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Auc area = {:.4f})'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "mlflow.log_metric(\"auc\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# def build_model(hp):\n",
    "#     model = keras.Sequential()\n",
    "    \n",
    "#     dropout_min  =  0\n",
    "#     dropout_max  =  0.6\n",
    "#     dropout_step =  0.1\n",
    "    \n",
    "#     for i in range(hp.Int('num_layers', 2, 10)):\n",
    "#         model.add(Dense(units=hp.Int('units' + str(i), min_value=32, max_value=512, step=32), activation='relu'))\n",
    "#         model.add(Dropout( hp.Float('dropout'+ str(i),min_value=0,max_value=0.5,step=0.1) ) )\n",
    "        \n",
    "#     model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',values=[1e-2, 1e-3, 1e-4])),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=['acc']\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# tuner = RandomSearch(\n",
    "#     build_model,\n",
    "#     objective='acc',\n",
    "#     max_trials=15,\n",
    "#     executions_per_trial=3,\n",
    "#     directory='my_dir',\n",
    "#     project_name='helloworld')\n",
    "\n",
    "# tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search_space_summary()\n",
    "# models = tuner.get_best_models(num_models=2)\n",
    "# tuner.results_summary()\n",
    "# print(tuner.get_best_hyperparameters()[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os \n",
    "\n",
    "current_dir = os.path.abspath('')\n",
    "transformer_path = \"data_transformer.joblib\"\n",
    "\n",
    "joblib.dump(transformer, transformer_path)\n",
    "\n",
    "mlflow.keras.log_model(model, '')\n",
    "mlflow.log_artifact(transformer_path, 'data')\n",
    "mlflow.log_artifact(os.path.abspath('') + '/keras_model.ipynb')\n",
    "\n",
    "os.remove(current_dir + '/' + transformer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.keras.load_model('mlruns/0/ab046c55dcd7400d91fa3cb4e4abf697/artifacts')\n",
    "transformer = joblib.load(\"mlruns/0/ab046c55dcd7400d91fa3cb4e4abf697/artifacts/data/data_transformer.joblib\")\n",
    "X_test = transformer.transform(train)\n",
    "print(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "transformer\n",
    "\n",
    "# joblib.dump(transformer, \"assets/data_transformer.joblib\")\n",
    "# model.save(\"assets/fraud_prediction_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis:\n",
    "https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd\n",
    "    \n",
    "https://www.kaggle.com/tarunpaparaju/how-to-survive-the-shakeup-don-t-overfit\n",
    "left on Ensembling\n",
    "\n",
    "\n",
    "IDEAS:\n",
    "    * check identity tabel\n",
    "    * improve data preperation\n",
    "    * categorical_features encoding using embedding\n",
    "    * autoencoders\n",
    "    * split data in a way that we only train on normal txs\n",
    "    * Ensembling\n",
    "    * oversampling/undersampling smoth\n",
    "    * K-fold Cross-Validation \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
